{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "xW17ZwGYgatK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from neuroCombat import neuroCombat\n",
        "\n",
        "try:\n",
        "    # Works if running from a Python script\n",
        "    current_file = os.path.abspath(__file__)\n",
        "except NameError:\n",
        "    # Fallback for Jupyter Notebook or IPython\n",
        "    current_file = os.path.abspath('')\n",
        "\n",
        "project_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(current_file))) ,'germany/FL/Experiments_1_2_eNKI/silo_Datasets')\n",
        "sys.path.append(project_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading data from /home/tanurima/germany/FL/Experiments_1_2_eNKI/silo_Datasets/CamCAN/Train_CamCAN.csv and /home/tanurima/germany/FL/Experiments_1_2_eNKI/silo_Datasets/eNKI/Train_eNKI.csv\n"
          ]
        }
      ],
      "source": [
        "silo = 'CamCAN' #or 'SALD'\n",
        "silo_path = os.path.join(project_dir,f'{silo}/Train_{silo}.csv')\n",
        "central_path = os.path.join(project_dir,f'eNKI/Train_eNKI.csv')\n",
        "\n",
        "print(f\"Loading data from {silo_path} and {central_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QoTuzXH-iSOV",
        "outputId": "26487648-6ae6-43ff-8fca-41c1ce7b7abf"
      },
      "outputs": [],
      "source": [
        "# Example: Load data from two scanners\n",
        "# Replace with your actual data paths\n",
        "df1 = pd.read_csv(silo_path)\n",
        "df2 = pd.read_csv(central_path)  # Shape: (features, subjects)\n",
        "\n",
        "# Get feature columns (assuming columns are named '1', '2', ..., '1073')\n",
        "feature_columns = [str(i) for i in range(1, 1074)]  # Generates ['1', '2', ..., '1073']\n",
        "#print('feature columns',feature_columns)\n",
        "# Convert to numpy arrays\n",
        "X_1 = np.transpose(df1[feature_columns].to_numpy()).astype(np.float32)  #Features matrix\n",
        "y_1 = np.transpose(df1['age'].to_numpy()).astype(np.float32).tolist()            #Target vector\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_2 = np.transpose(df2[feature_columns].to_numpy()).astype(np.float32)  #Features matrix\n",
        "y_2 = np.transpose(df2['age'].to_numpy()).astype(np.float32).tolist()            #Target vector\n",
        "\n",
        "# Combine datasets horizontally (columns = all subjects from both scanners)\n",
        "data_combined = np.hstack([X_1, X_2])  # Shape: (features, subjects_total)\n",
        "age_list = y_1 + y_2  # Shape: (subjects_total,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "khK7pzSn5UmB"
      },
      "outputs": [],
      "source": [
        "# Example: 5 subjects from scanner1 and 5 from scanner2\n",
        "n_scanner1 = X_1.shape[1]  # Number of subjects in scanner1\n",
        "n_scanner2 = X_2.shape[1]  # Number of subjects in scanner2\n",
        "\n",
        "covars = {\n",
        "    'batch': [1]*n_scanner1 + [2]*n_scanner2,  # Scanner IDs\n",
        "    'age':  age_list  # Example continuous covariate [30,25,40,35,28, 32,45,50,38,42]\n",
        "}\n",
        "covars = pd.DataFrame(covars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTDewdkS64nI",
        "outputId": "ba383750-bd0f-4936-efd0-0e50c11f9123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[neuroCombat] Creating design matrix\n",
            "[neuroCombat] Standardizing data across features\n",
            "[neuroCombat] Fitting L/S model and finding priors\n",
            "[neuroCombat] Finding parametric adjustments\n",
            "[neuroCombat] Final adjustment of data\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1073, 1315)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Harmonize the data\n",
        "harmonized_output = neuroCombat(\n",
        "    dat=data_combined,\n",
        "    covars=covars,\n",
        "    batch_col='batch',              # Column name for scanner IDs\n",
        "    categorical_cols=[],    # Specify categorical variables\n",
        "    # Optional parameters:\n",
        "    eb=True,             # Use Empirical Bayes (recommended)\n",
        "    parametric=True,      # Parametric adjustment (default)\n",
        "    mean_only=False,      # Adjust both mean and variance (default)\n",
        "    ref_batch=None        # Harmonize to overall average (set to 1 or 2 to use a scanner as reference)\n",
        ")\n",
        "\n",
        "# Extract harmonized data\n",
        "data_harmonized = harmonized_output[\"data\"]\n",
        "data_harmonized.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYNZWDEe7dT2",
        "outputId": "1a3cc48f-9644-4038-c0f7-f907351f7332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "feature columns: ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '98', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '115', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '142', '143', '144', '145', '146', '147', '148', '149', '150', '151', '152', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '179', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '191', '192', '193', '194', '195', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '222', '223', '224', '225', '226', '227', '228', '229', '230', '231', '232', '233', '234', '235', '236', '237', '238', '239', '240', '241', '242', '243', '244', '245', '246', '247', '248', '249', '250', '251', '252', '253', '254', '255', '256', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '268', '269', '270', '271', '272', '273', '274', '275', '276', '277', '278', '279', '280', '281', '282', '283', '284', '285', '286', '287', '288', '289', '290', '291', '292', '293', '294', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '314', '315', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '327', '328', '329', '330', '331', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '346', '347', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378', '379', '380', '381', '382', '383', '384', '385', '386', '387', '388', '389', '390', '391', '392', '393', '394', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '406', '407', '408', '409', '410', '411', '412', '413', '414', '415', '416', '417', '418', '419', '420', '421', '422', '423', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '450', '451', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '463', '464', '465', '466', '467', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '482', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '494', '495', '496', '497', '498', '499', '500', '501', '502', '503', '504', '505', '506', '507', '508', '509', '510', '511', '512', '513', '514', '515', '516', '517', '518', '519', '520', '521', '522', '523', '524', '525', '526', '527', '528', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', '591', '592', '593', '594', '595', '596', '597', '598', '599', '600', '601', '602', '603', '604', '605', '606', '607', '608', '609', '610', '611', '612', '613', '614', '615', '616', '617', '618', '619', '620', '621', '622', '623', '624', '625', '626', '627', '628', '629', '630', '631', '632', '633', '634', '635', '636', '637', '638', '639', '640', '641', '642', '643', '644', '645', '646', '647', '648', '649', '650', '651', '652', '653', '654', '655', '656', '657', '658', '659', '660', '661', '662', '663', '664', '665', '666', '667', '668', '669', '670', '671', '672', '673', '674', '675', '676', '677', '678', '679', '680', '681', '682', '683', '684', '685', '686', '687', '688', '689', '690', '691', '692', '693', '694', '695', '696', '697', '698', '699', '700', '701', '702', '703', '704', '705', '706', '707', '708', '709', '710', '711', '712', '713', '714', '715', '716', '717', '718', '719', '720', '721', '722', '723', '724', '725', '726', '727', '728', '729', '730', '731', '732', '733', '734', '735', '736', '737', '738', '739', '740', '741', '742', '743', '744', '745', '746', '747', '748', '749', '750', '751', '752', '753', '754', '755', '756', '757', '758', '759', '760', '761', '762', '763', '764', '765', '766', '767', '768', '769', '770', '771', '772', '773', '774', '775', '776', '777', '778', '779', '780', '781', '782', '783', '784', '785', '786', '787', '788', '789', '790', '791', '792', '793', '794', '795', '796', '797', '798', '799', '800', '801', '802', '803', '804', '805', '806', '807', '808', '809', '810', '811', '812', '813', '814', '815', '816', '817', '818', '819', '820', '821', '822', '823', '824', '825', '826', '827', '828', '829', '830', '831', '832', '833', '834', '835', '836', '837', '838', '839', '840', '841', '842', '843', '844', '845', '846', '847', '848', '849', '850', '851', '852', '853', '854', '855', '856', '857', '858', '859', '860', '861', '862', '863', '864', '865', '866', '867', '868', '869', '870', '871', '872', '873', '874', '875', '876', '877', '878', '879', '880', '881', '882', '883', '884', '885', '886', '887', '888', '889', '890', '891', '892', '893', '894', '895', '896', '897', '898', '899', '900', '901', '902', '903', '904', '905', '906', '907', '908', '909', '910', '911', '912', '913', '914', '915', '916', '917', '918', '919', '920', '921', '922', '923', '924', '925', '926', '927', '928', '929', '930', '931', '932', '933', '934', '935', '936', '937', '938', '939', '940', '941', '942', '943', '944', '945', '946', '947', '948', '949', '950', '951', '952', '953', '954', '955', '956', '957', '958', '959', '960', '961', '962', '963', '964', '965', '966', '967', '968', '969', '970', '971', '972', '973', '974', '975', '976', '977', '978', '979', '980', '981', '982', '983', '984', '985', '986', '987', '988', '989', '990', '991', '992', '993', '994', '995', '996', '997', '998', '999', '1000', '1001', '1002', '1003', '1004', '1005', '1006', '1007', '1008', '1009', '1010', '1011', '1012', '1013', '1014', '1015', '1016', '1017', '1018', '1019', '1020', '1021', '1022', '1023', '1024', '1025', '1026', '1027', '1028', '1029', '1030', '1031', '1032', '1033', '1034', '1035', '1036', '1037', '1038', '1039', '1040', '1041', '1042', '1043', '1044', '1045', '1046', '1047', '1048', '1049', '1050', '1051', '1052', '1053', '1054', '1055', '1056', '1057', '1058', '1059', '1060', '1061', '1062', '1063', '1064', '1065', '1066', '1067', '1068', '1069', '1070', '1071', '1072', '1073', 'age']\n",
            "Harmonized data for scanner1:\n",
            "(520, 1073)\n",
            "Target variable for scanner1:\n",
            "(520, 1)\n",
            "harmonized data shape: (520, 1074)\n"
          ]
        }
      ],
      "source": [
        "# Split harmonized data back into original scanners (if needed)\n",
        "harmonized_scanner1 = data_harmonized[:, :n_scanner1]\n",
        "harmonized_scanner2 = data_harmonized[:, n_scanner1:]\n",
        "\n",
        "y_1 = np.transpose(df1['age'].to_numpy()).astype(np.float32).tolist()            #Target vector\n",
        "y_2 = np.transpose(df2['age'].to_numpy()).astype(np.float32).tolist()            #Target vector\n",
        "\n",
        "# Move this line before creating harmonized_combined_data\n",
        "feature_columns.append('age')\n",
        "print('feature columns:',feature_columns)\n",
        "\n",
        "print(\"Harmonized data for scanner1:\")\n",
        "print(harmonized_scanner1.T.shape)\n",
        "print(  \"Target variable for scanner1:\")\n",
        "age = np.array(y_1).reshape(-1, 1)\n",
        "print(age.shape)\n",
        "\n",
        "# Concatenate the arrays horizontally (adding target as an extra column)\n",
        "harmonized_combined_data = np.concatenate((harmonized_scanner1.T, age), axis=1)\n",
        "print('harmonized data shape:',harmonized_combined_data.shape)\n",
        "harmonized_train = pd.DataFrame(harmonized_combined_data, columns=feature_columns)\n",
        "harmonized_path = silo_path.replace('Train','Train_harmonized')\n",
        "harmonized_train.to_csv(harmonized_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TilcnlrxI_p7"
      },
      "outputs": [],
      "source": [
        "def apply_combat_harmonization(test_path, combat_params, batch_col='batch', categorical_cols=[]):\n",
        "    \"\"\"\n",
        "    Apply pre-trained Combat harmonization to new data.\n",
        "\n",
        "    Args:\n",
        "        test_path (str): Path to new test data CSV\n",
        "        combat_params (dict): Saved Combat parameters from training, containing:\n",
        "                              - \"estimates\": Model parameters\n",
        "                              - \"batch_col\": Batch column name (e.g., \"batch\")\n",
        "                              - \"categorical_cols\": List of categorical covariates\n",
        "    \"\"\"\n",
        "    # Load test data\n",
        "    test_data = pd.read_csv(test_path)\n",
        "\n",
        "    # Clean data (drop rows with missing age)\n",
        "    test_data_clean = test_data.dropna(subset=['age'], axis=0)\n",
        "\n",
        "    # Check if test data is empty after cleaning\n",
        "    if test_data_clean.empty:\n",
        "        raise ValueError(\"Test data is empty after dropping rows with missing 'age'.\")\n",
        "\n",
        "    # 1. Add batch column to test_data_clean\n",
        "    # Assign all test data to a single batch (e.g., batch=1)\n",
        "    test_data_clean['batch'] = 1  # Replace 1 with your desired batch label\n",
        "\n",
        "    # 2. Extract covariates (MUST include 'batch' and other training covariates)\n",
        "    required_covars = ['age', 'batch']  # Add other covariates used during training\n",
        "    covars_test = test_data_clean[required_covars]\n",
        "\n",
        "    # 3. Extract features (columns '1' to '1073')\n",
        "    new_data = test_data_clean.loc[:, '1':'1073']\n",
        "\n",
        "    # Transpose to (features x subjects) format\n",
        "    new_data_t = new_data.T\n",
        "\n",
        "    # 4. Apply pre-trained Combat parameters\n",
        "    # Instead of passing estimates directly to neuroCombat,\n",
        "    # you would need to modify the input data (new_data_t)\n",
        "    # based on combat_params[\"estimates\"] before calling neuroCombat.\n",
        "    # For instance, you might apply the estimated batch effects\n",
        "    # to new_data_t using operations like addition, subtraction,\n",
        "    # multiplication, or division, as appropriate based on the\n",
        "    # specific Combat model you are using.\n",
        "\n",
        "    # Example: Let's assume you need to add a batch effect\n",
        "    # This is a simplified example, you need to adapt it based on your\n",
        "    # actual model and the content of combat_params[\"estimates\"]\n",
        "    # batch_effect = combat_params[\"estimates\"][\"batch_effect\"]  # Replace with actual key\n",
        "    # new_data_t = new_data_t + batch_effect\n",
        "\n",
        "    # Since neuroCombat doesn't accept 'estimates', remove it from the call:\n",
        "    harmonized = neuroCombat(\n",
        "        dat=new_data_t,\n",
        "        covars=covars_test,\n",
        "        batch_col=batch_col,\n",
        "        categorical_cols=categorical_cols,\n",
        "        # estimates=combat_params[\"estimates\"]  # Remove this line\n",
        "    )[\"data\"]\n",
        "\n",
        "    # Convert back to DataFrame and add age labels\n",
        "    harmonized_df = pd.DataFrame(harmonized.T, columns=new_data.columns)\n",
        "    harmonized_df['age'] = test_data_clean['age'].values\n",
        "\n",
        "    return harmonized_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lCbGbhHAqlE",
        "outputId": "36137c97-a2bd-4875-9bbb-01471f9eda5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[neuroCombat] Creating design matrix\n",
            "[neuroCombat] Standardizing data across features\n",
            "[neuroCombat] Fitting L/S model and finding priors\n",
            "[neuroCombat] Finding parametric adjustments\n",
            "[neuroCombat] Final adjustment of data\n"
          ]
        }
      ],
      "source": [
        "# Call apply_combat_harmonization()\n",
        "#test_path = '/content/CamCAN_test.csv'  # Path to your test data CSV\n",
        "# After initial harmonization\n",
        "combat_params = {\n",
        "    \"estimates\": harmonized_output[\"estimates\"],\n",
        "    \"batch_col\": \"batch\",\n",
        "    \"categorical_cols\": []\n",
        "}\n",
        "silo_test_path = silo_path.replace('Train','Test')\n",
        "harmonized_test_df = apply_combat_harmonization(\n",
        "    test_path=silo_test_path,\n",
        "    combat_params=combat_params,\n",
        "    batch_col='batch',\n",
        "    categorical_cols=[]\n",
        ")\n",
        "harmonized_test_path = silo_test_path.replace('Test','Test_harmonized')\n",
        "harmonized_test_df.to_csv(harmonized_test_path)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "test_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
